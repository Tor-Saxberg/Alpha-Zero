{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9407,
     "status": "ok",
     "timestamp": 1568632816317,
     "user": {
      "displayName": "Tor Saxberg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDMqlWuXygkDE3LzKrVH8_fetTuNkbD0ke0p24O=s64",
      "userId": "17428565500638805826"
     },
     "user_tz": 420
    },
    "id": "6K8dl_WCreux",
    "outputId": "b2b7a1ef-d508-49c6-b8e5-2a2de14bfa0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "try:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUmNG0KUrKyp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import enum\n",
    "from termcolor import colored\n",
    "from copy import copy\n",
    "from pprint import pprint\n",
    "\n",
    "# noinspection PyArgumentList\n",
    "\n",
    "class Connect4Env:\n",
    "    def __init__(self, board=None):\n",
    "        \"\"\"initialize environment\"\"\"\n",
    "        self.name = 'Connect4'\n",
    "        self.board_size = (6,7)\n",
    "        self.action_size = 7\n",
    "\n",
    "        if board is None:\n",
    "            self.board = np.zeros(self.board_size).astype('int')\n",
    "            self.board[:] = 2 # for empty\n",
    "        else: self.board = board\n",
    "\n",
    "        self.turn = 0\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        self.winning_moves = np.zeros(self.board_size).astype('int')\n",
    "        self.last_move = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"reset environment state\"\"\"\n",
    "        self.board[:] = 2 # for empty\n",
    "        self.turn = 0\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        self.winning_moves = np.zeros(self.board_size).astype('int')\n",
    "        self.last_move = None\n",
    "\n",
    "    def player_turn(self):\n",
    "        \"\"\"return current player: 0 or 1\"\"\"\n",
    "        return self.turn % 2 # O goes first\n",
    "\n",
    "    def last(self):\n",
    "        return self.last_move[1]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"increment player, then play move\"\"\"\n",
    "        if self.done:\n",
    "            breakpoint()\n",
    "            raise ValueError(\"game already over\"); return\n",
    "        if action not in self.legal_moves():\n",
    "            breakpoint()\n",
    "            raise ValueError(\"illegal move\"); return\n",
    "\n",
    "        self.turn += 1\n",
    "        for i in range(5,-1,-1):\n",
    "            if self.board[i][action] == 2:\n",
    "                self.board[i][action] = self.player_turn()\n",
    "                self.last_move = (i, action)\n",
    "                break\n",
    "\n",
    "        self.check_for_fours(action)\n",
    "        if self.turn >= 42:\n",
    "            self.done = True\n",
    "        if self is None: print(\"Nonetype Connec4tEnv\")\n",
    "        return self\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return [col for col,_ in enumerate(self.board[0]) if self.board[0][col]==2]\n",
    "\n",
    "    def augment(self, win):\n",
    "        \"\"\"augment data, and assign values\"\"\"\n",
    "        boards = []; Qs = []; policies = []\n",
    "\n",
    "        boards.append(self.board)\n",
    "        Qs.append(win)\n",
    "        policies.append(self.pi)\n",
    "\n",
    "        \"\"\"flip board and policies for symmetry\"\"\"\n",
    "        boards.append(np.flip(self.board, axis=1))\n",
    "        Qs.append(win)\n",
    "        policies.append(np.flip(self.pi))\n",
    "        win = -win # 2nd to last player lost\n",
    "        return boards, Qs, policies\n",
    "\n",
    "\n",
    "\n",
    "    def check_for_fours(self, action):\n",
    "        if self.vertical_check(action): self.done = True\n",
    "        elif self.horizontal_check(action): self.done = True\n",
    "        elif self.diagonal(action): self.done = True\n",
    "        if self.done == True: self.winner = self.player_turn()\n",
    "\n",
    "    def vertical_check(self, action):\n",
    "        \"\"\"return winner if vertical 4-in-a-row\"\"\"\n",
    "        for row in range(6):\n",
    "            if self.board[row][action] != 2: break\n",
    "        count = 0\n",
    "\n",
    "        for i in range(row, 6):\n",
    "            if self.board[i][action] == self.player_turn():\n",
    "                count += 1\n",
    "                self.winning_moves[i][action] = 1\n",
    "            else: break\n",
    "        if count >= 4: return True\n",
    "        else: self.winning_moves[:] = 0; return False\n",
    "\n",
    "    def horizontal_check(self, action):\n",
    "        \"\"\"return winner if horizontal 4-in-a-row\"\"\"\n",
    "        count = 0\n",
    "\n",
    "        # find row of last move\n",
    "        for i in range(6):\n",
    "            if self.board[i][action] != 2: break\n",
    "        # look left of last move\n",
    "        for j in range(action, -1, -1):\n",
    "            if self.board[i][j] == self.player_turn():\n",
    "                count += 1\n",
    "                self.winning_moves[i][j] = 1\n",
    "            else: break\n",
    "        # look right of last move:\n",
    "        for j in range(action+1,7):\n",
    "            if self.board[i][j] == self.player_turn():\n",
    "                count += 1\n",
    "                self.winning_moves[i][j] = 1\n",
    "            else: break\n",
    "\n",
    "        # if there was a 4-pattern, return winner\n",
    "        if count >= 4: return True\n",
    "        else: self.winning_moves[:] = 0; return False\n",
    "\n",
    "    def diagonal(self, action):\n",
    "        \"\"\"return winnfer if diagonal 4-in-a-row\"\"\"\n",
    "        # i = row of last play, top to bottom\n",
    "        for i in range(6):\n",
    "            if self.board[i][action] != 2: break\n",
    "\n",
    "        # check main diagnoal\n",
    "        count = 0\n",
    "        # check left and up\n",
    "        ii = i\n",
    "        jj = action\n",
    "        while ii >= 0 and jj >= 0:\n",
    "            if self.board[ii][jj] == self.player_turn():\n",
    "                count += 1\n",
    "                self.winning_moves[ii][jj] = 1\n",
    "            else: break\n",
    "            ii -= 1\n",
    "            jj -= 1\n",
    "        # check right and down \n",
    "        ii = i\n",
    "        jj = action\n",
    "        while ii < 6 and jj < 7:\n",
    "            if self.board[ii][jj] == self.player_turn():\n",
    "                count += 1\n",
    "                self.winning_moves[ii][jj] = 1\n",
    "            else: break\n",
    "            ii += 1\n",
    "            jj += 1\n",
    "        if count > 4: return True # handling counting action twice\n",
    "\n",
    "        #check off diagnoal\n",
    "        count = 0\n",
    "        self.winning_moves[:] = 0\n",
    "        # check right and up\n",
    "        ii = i\n",
    "        jj = action\n",
    "        while ii >= 0 and jj < 7:\n",
    "            if self.board[ii][jj] == self.player_turn():\n",
    "                count += 1\n",
    "                self.winning_moves[ii][jj] = 1\n",
    "            else: break\n",
    "            ii -= 1\n",
    "            jj += 1\n",
    "        # check left and down \n",
    "        ii = i\n",
    "        jj = action\n",
    "        while ii < 6 and jj >= 0:\n",
    "            if self.board[ii][jj] == self.player_turn():\n",
    "                count += 1\n",
    "                self.winning_moves[ii][jj] = 1\n",
    "            else: break\n",
    "            ii += 1\n",
    "            jj -= 1\n",
    "        if count > 4: return True\n",
    "        else: self.winning_moves[:] = 0; return False\n",
    "\n",
    "    def render(self, indent = 1):\n",
    "        print('\\nturn: ' + str(self.turn) + ', last: ' + str(self.last_move))\n",
    "\n",
    "        for i in range(6):\n",
    "            print(\"\\t\"*indent, end=\"\")\n",
    "            for j in range(7):\n",
    "                if self.board[i][j] == 0:\n",
    "                    if self.last_move == (i,j): print(colored('| X', 'green'), end=\" \")\n",
    "                    elif self.winning_moves[i][j]: print(colored('| X', 'red'), end=\" \")\n",
    "                    else: print(\"| \" + 'X', end=\" \")\n",
    "                elif self.board[i][j] == 1:\n",
    "                    if self.last_move == (i,j): print(colored('| O', 'green'), end=\" \")\n",
    "                    elif self.winning_moves[i][j]: print(colored('| O','red'), end=\" \")\n",
    "                    else: print(\"| \" + 'O', end=\" \")\n",
    "                else: print(\"|  \", end=\" \")\n",
    "                #print(\"| \" + str(self.board[i][j]), end=\" \")\n",
    "            print(\"|\")\n",
    "        print(\"\\t  _   _   _   _   _   _   _ \")\n",
    "        print(\"\\t  0   1   2   3   4   5   6 \")\n",
    "        if self.done:\n",
    "            print(\"Game Over!\")\n",
    "            if self.winner == 0:\n",
    "                print(\"X is the winner\")\n",
    "            elif self.winner == 1:\n",
    "                print(\"O is the winner\")\n",
    "            else:\n",
    "                print(\"draw game\")\n",
    "\n",
    "    def __copy__(self):\n",
    "        \"\"\"copy board\"\"\"\n",
    "        new = type(self)()\n",
    "        new.board = copy(self.board)\n",
    "        new.turn = self.turn\n",
    "        new.done = self.done\n",
    "        new.winner = self.winner\n",
    "        new.winning_moves = copy(self.winning_moves)\n",
    "        new.last_move = self.last_move\n",
    "        return new\n",
    "\n",
    "    def __repr__(self):\n",
    "        #self.render()\n",
    "        self.render()\n",
    "        return '\\nturn: {},last: {}, hash: {}'.format( \\\n",
    "        self.turn, self.last_move, self.__hash__() )\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"Nodes must be hashable\"\n",
    "        return hash(tuple(self.board.flatten() ))\n",
    "\n",
    "    def __eq__(node1, node2):\n",
    "        \"Nodes must be comparable\"\n",
    "        if node1 is None: return True\n",
    "        if node1 is not None and node2 is None: return False\n",
    "        return np.array_equal(node1.board, node2.board)\n",
    "\n",
    "    def __gt__(node1, node2):\n",
    "        return node1.turn > node2.turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQlnBfWppc7g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import enum\n",
    "from termcolor import colored\n",
    "from copy import copy\n",
    "from pprint import pprint\n",
    "\n",
    "# noinspection PyArgumentList\n",
    "\n",
    "class TictactoeEnv:\n",
    "    def __init__(self, board=None):\n",
    "        \"\"\"initialize environment\"\"\"\n",
    "        self.name = 'tictactoe'\n",
    "        self.board_size = (3,3)\n",
    "        self.action_size = 9\n",
    "\n",
    "        if board is None:\n",
    "            self.board = np.zeros(self.board_size).astype('int')\n",
    "            self.board[:] = 0 # for empty\n",
    "        else: self.board = board\n",
    "\n",
    "        self.turn = 0\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        self.winning_moves = np.zeros(self.board_size)\n",
    "        self.last_move = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"reset environment state\"\"\"\n",
    "        self.board[:] = 0 # for empty\n",
    "        self.turn = 0\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        self.winning_moves = np.zeros(self.board_size)\n",
    "        self.last_move = None\n",
    "\n",
    "    def player_turn(self):\n",
    "        \"\"\"return current player: 0 or 1\"\"\"\n",
    "        return self.turn % 2 + 1 # {1,2}, 1 goes first\n",
    "\n",
    "    def last(self):\n",
    "        return self.last_move[0]*3 + self.last_move[1]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"increment player, then play move\"\"\"\n",
    "        action = int(action) \n",
    "        if self.done:\n",
    "            raise ValueError(\"game already over\"); return\n",
    "        if type(action) != int:\n",
    "            raise ValueError('bad action'); return\n",
    "        if action not in self.legal_moves():\n",
    "            raise ValueError(\"illegal move\"); return\n",
    "        \n",
    "        act = [0,0]; act[0] = int(action/3); act[1] = action%3\n",
    "        self.turn += 1\n",
    "\n",
    "        if self.board[act[0]][act[1]] == 0:\n",
    "            self.board[act[0]][act[1]] = self.player_turn()\n",
    "            self.last_move = act\n",
    "\n",
    "        self.check(act)\n",
    "        if self.turn >= 9:\n",
    "            self.done = True\n",
    "        if self is None: print(\"Nonetype TictactoeEnv\")\n",
    "        return self\n",
    "\n",
    "    def legal_moves(self):\n",
    "        row,col = np.where(self.board==0)\n",
    "        return [row*3 + col][0]\n",
    "\n",
    "\n",
    "    def augment(self, board, pi, win):\n",
    "        \"\"\"augment data, and assign values\"\"\"\n",
    "        boards = []; Qs = []; policies = []\n",
    "\n",
    "        boards.append(board)\n",
    "        Qs.append(win)\n",
    "        policies.append(pi)\n",
    "\n",
    "        \"\"\"flip horizontally\"\"\"\n",
    "        boards.append(np.flip(board, axis=0))\n",
    "        Qs.append(win)\n",
    "        policies.append(np.flip(pi))\n",
    "\n",
    "        \"\"\"flip vertically\"\"\"\n",
    "        boards.append(np.flip(board, axis=1))\n",
    "        Qs.append(win)\n",
    "        policies.append(np.flip(pi))\n",
    "\n",
    "        \"\"\"flip horizontally and vertically\"\"\"\n",
    "        temp = np.flip(board, axis=0)\n",
    "        boards.append(np.flip(temp, axis=1))\n",
    "        Qs.append(win)\n",
    "        policies.append(np.flip(pi))\n",
    "\n",
    "        return boards, Qs, policies\n",
    "\n",
    "    def _separate_players(self):\n",
    "        \"\"\"split board into player1 board and player2 board.\\n\n",
    "        return player1_board, player2_board, next_player_turn\"\"\"\n",
    "        player1_board=np.zeros(self.board_size) \n",
    "        player2_board=np.zeros(self.board_size) \n",
    "        next_player = np.ones(self.board_size) * self.player_turn() - 1 # {0,1}\n",
    "        for i in range(self.board_size[0]):\n",
    "            for j in range(self.board_size[1]):\n",
    "                if self.board[i][j] == 1:\n",
    "                    player1_board[i][j] = 1\n",
    "                elif self.board[i][j] == 2:\n",
    "                    player2_board[i][j] = 1\n",
    "        return player1_board, player2_board, next_player\n",
    "\n",
    "    def check(self, action):\n",
    "        if self.vertical_check(action): self.done = True\n",
    "        elif self.horizontal_check(action): self.done = True\n",
    "        elif self.diagonal(action): self.done = True\n",
    "        if self.done == True: self.winner = self.player_turn()\n",
    "\n",
    "    def vertical_check(self, action):\n",
    "        \"\"\"return winner if vertical 3-in-a-row\"\"\"\n",
    "        count = 0\n",
    "        for i in range(3):\n",
    "            if self.board[i][action[1]] == self.player_turn():\n",
    "                count += 1\n",
    "                self.winning_moves[i][action[1]] = 1\n",
    "            else: break\n",
    "        if count == 3: return True\n",
    "        else: self.winning_moves[:] = 0; return False\n",
    "\n",
    "    def horizontal_check(self, action):\n",
    "        \"\"\"return winner if horizontal 3-in-a-row\"\"\"\n",
    "        count = 0\n",
    "        for i in range(3):\n",
    "            if self.board[action[0]][i] == self.player_turn():\n",
    "                count += 1\n",
    "                self.winning_moves[action[0]][i] = 1\n",
    "            else: break\n",
    "        if count >= 3: return True\n",
    "        else: self.winning_moves[:] = 0; return False\n",
    "\n",
    "    def diagonal(self, action):\n",
    "        \"\"\"return winnfer if diagonal 4-in-a-row\"\"\"\n",
    "        # check main diagnoal\n",
    "        if self.board[0][0] == self.board[1][1] == self.board[2][2] == self.player_turn():\n",
    "            self.winning_moves[0][0] = self.winning_moves[1][1] = self.winning_moves[2][2] = 1\n",
    "            return True\n",
    "        #check off diagnoal\n",
    "        if self.board[0][2] == self.board[1][1] == self.board[2][0] == self.player_turn():\n",
    "            self.winning_moves[0][2] = self.winning_moves[1][1] = self.winning_moves[2][0] = 1\n",
    "            return True\n",
    "\n",
    "    def render(self):\n",
    "        print('\\nturn: ' + str(self.turn) + ', last: ' + str(self.last_move))\n",
    "\n",
    "        for i in range(3):\n",
    "            print(f\"\\t{i} \", end=\"\")\n",
    "            for j in range(3):\n",
    "                if self.board[i][j] == 1:\n",
    "                    if self.last_move == [i,j]: print(colored('| X', 'green'), end=\" \")\n",
    "                    elif self.winning_moves[i][j]: print(colored('| X', 'red'), end=\" \")\n",
    "                    else: print(\"| \" + 'X', end=\" \")\n",
    "                elif self.board[i][j] == 2:\n",
    "                    if self.last_move == [i,j]: print(colored('| O', 'green'), end=\" \")\n",
    "                    elif self.winning_moves[i][j]: print(colored('| O','red'), end=\" \")\n",
    "                    else: print(\"| \" + 'O', end=\" \")\n",
    "                else: print(\"|  \", end=\" \")\n",
    "                #print(\"| \" + str(self.board[i][j]), end=\" \")\n",
    "            print(\"|\")\n",
    "\n",
    "        print(\"\\t   _   _   _ \")\n",
    "        print(\"\\t   0   1   2 \")\n",
    "        if self.done:\n",
    "            print(\"Game Over!\")\n",
    "            if self.winner == 1:\n",
    "                print(\"X is the winner\")\n",
    "            elif self.winner == 2:\n",
    "                print(\"O is the winner\")\n",
    "            else:\n",
    "                print(\"draw game\")\n",
    "\n",
    "    def __copy__(self):\n",
    "        \"\"\"copy board\"\"\"\n",
    "        new = type(self)()\n",
    "        new.board = copy(self.board)\n",
    "        new.turn = self.turn\n",
    "        new.done = self.done\n",
    "        new.winner = self.winner\n",
    "        new.winning_moves = copy(self.winning_moves)\n",
    "        new.last_move = self.last_move\n",
    "        return new\n",
    "\n",
    "    def __repr__(self):\n",
    "        #self.render()\n",
    "        self.render()\n",
    "        return f'\\nturn: {self.turn},last: {self.last_move}, id: {id(self)}'\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"Nodes must be hashable\"\n",
    "        return hash(tuple(self.board.flatten() ))\n",
    "\n",
    "    def __eq__(node1, node2):\n",
    "        \"Nodes must be comparable\"\n",
    "        if node1 is None: return True\n",
    "        if node1 is not None and node2 is None: return False\n",
    "        return np.array_equal(node1.board, node2.board)\n",
    "\n",
    "    def __gt__(node1, node2):\n",
    "        return node1.turn > node2.turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTpxOn5Ysh8w"
   },
   "outputs": [],
   "source": [
    "import os, sys; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from time import time\n",
    "from tensorflow.compat.v1 import logging; logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, BatchNormalization, LeakyReLU, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import mse, categorical_accuracy\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class DNN():\n",
    "    def __init__(self, env, net='dnn'):\n",
    "        print('dnn initializing')\n",
    "        self.game = env.name\n",
    "        self.net = net\n",
    "        self.action_size = env.action_size # an int\n",
    "        self.board_size = env.board_size # a tuple\n",
    "\n",
    "        inputs = Input(shape=(self.board_size[0], self.board_size[1]*3) )\n",
    "        net = Flatten()(inputs)\n",
    "        Q = Dense(1, activation='sigmoid', name='Q_layer')(net)\n",
    "\n",
    "        size = self.board_size[0] * self.board_size[1] * self.action_size\n",
    "        while size > self.action_size * 2:\n",
    "            net = Dense(int(size))(net)\n",
    "            net = BatchNormalization(axis=1)(net)\n",
    "            net = LeakyReLU(alpha=0.3)(net)\n",
    "            net = Dropout(rate=0.3)(net)\n",
    "            size /= 2\n",
    "\n",
    "        pi = Dense(self.action_size, activation='softmax', name='pi_layer')(net) \n",
    "\n",
    "        self.model = Model(inputs=inputs, outputs=[Q, pi])\n",
    "        self.model.compile(loss=['mean_squared_error','categorical_crossentropy'], \n",
    "                           optimizer=Adam(0.001))\n",
    "\n",
    "        \"\"\"\n",
    "        print(self.model.summary())\n",
    "        plot_model(self.model, to_file='model_plot.png', \n",
    "                show_shapes=True, show_layer_names=True)\n",
    "        img = Image.open('model_plot.png')\n",
    "        img.show()\n",
    "        \"\"\"\n",
    "\n",
    "    def train(self, examples, virtual, epoch=0):\n",
    "        \"\"\"train network and reeturn win_rate\"\"\"\n",
    "        if not virtual:\n",
    "            \"\"\"train on path batch\"\"\"\n",
    "            count = 0\n",
    "            boards_list = []; Qs_list = []; policies_list = []\n",
    "            for example in examples: \n",
    "                boards, Qs, policies = example\n",
    "                boards_list.extend(boards)\n",
    "                Qs_list.extend(Qs)\n",
    "                policies_list.extend(policies)\n",
    "\n",
    "            boards = np.reshape(boards_list, (-1,*boards[0].shape) )\n",
    "            Qs = np.reshape(Qs_list, (-1,1))\n",
    "            policies = np.reshape(policies_list, (-1,self.action_size))\n",
    "            count += len(boards) # batch size, for printing\n",
    "            start = time()\n",
    "\n",
    "            checkpointer = ModelCheckpoint(filepath=self.save_checkpoint(epoch),\n",
    "                                           save_weights_only=True,\n",
    "                                           save_best_only=True,\n",
    "                                           #monitor='loss', mode='min',\n",
    "                                           verbose=0)\n",
    "            tensorboard = TensorBoard(log_dir=f'./logging/{self.game}_dnn_{epoch}',\n",
    "                                      histogram_freq=10,\n",
    "                                      write_images=True,\n",
    "                                      batch_size=boards.size,\n",
    "                                      update_freq='batch')\n",
    "\n",
    "            self.model.fit(x = boards, y = [Qs, policies],\n",
    "                           validation_split=0.15,\n",
    "                           batch_size=boards.size,\n",
    "                           epochs = 50, shuffle=False, verbose=0,\n",
    "                           callbacks=[checkpointer, tensorboard])\n",
    "\n",
    "            end = time()\n",
    "            print(f\"fitting {count} boards: {end - start}\")\n",
    "            sys.stdout.flush()\n",
    "            #self.save_checkpoint(epoch=epoch)\n",
    "\n",
    "    def save_checkpoint(self, epoch=0): \n",
    "            folder='./checkpoints'\n",
    "            filename=f'{self.game}_dnn_{epoch}.hdf5'\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            if not os.path.exists(folder):\n",
    "                print(f\"Making Directory {folder}\")\n",
    "                os.mkdir(folder)\n",
    "            print(f'saving to {filepath}')\n",
    "            #self.model.save_weights(filepath)\n",
    "            return filepath\n",
    "\n",
    "    def load_checkpoint(self, epoch=0):\n",
    "        folder='./checkpoints'\n",
    "        filename=f'{self.game}_dnn_{epoch}.hdf5'\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"No model in path {filepath}\")\n",
    "            return\n",
    "        print(f'loading from {filepath}')\n",
    "        self.model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jau4qNWFsj9e"
   },
   "outputs": [],
   "source": [
    "import os, sys; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from tensorflow.compat.v1 import logging; logging.set_verbosity(logging.ERROR)\n",
    "from time import time\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, Conv2D, BatchNormalization, Activation, Flatten, Dense, Dropout\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import mse, categorical_accuracy\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class CNN():\n",
    "    def __init__(self, env, net='blank'):\n",
    "        self.game = env.name\n",
    "        self.net = net\n",
    "        self.board_size = env.board_size\n",
    "        self.action_size = env.action_size\n",
    "\n",
    "        inputs = Input(shape=(self.board_size[0], self.board_size[1]*3) )\n",
    "        net = Reshape((self.board_size[0], self.board_size[1]*3, 1) )(inputs)\n",
    "        for i in range(3):\n",
    "            net = Conv2D(32, kernel_size=3, strides=1, padding='same', \n",
    "                    kernel_initializer=RandomUniform() )(net)\n",
    "            net = BatchNormalization(axis=3)(net)\n",
    "            net = Activation('relu')(net)\n",
    "        net = Flatten()(net)\n",
    "        for i in range(2):\n",
    "            net = Dense(int(1024/(4**i)), \n",
    "                    kernel_initializer=RandomUniform()  )(net)\n",
    "            net = BatchNormalization(axis=1)(net)\n",
    "            net = Activation('relu')(net)\n",
    "            net = Dropout(rate=0.3)(net)\n",
    "        Q = Dense(1, activation='sigmoid', \n",
    "                kernel_initializer=RandomUniform(), name='Q')(net)\n",
    "        pi = Dense(self.action_size, activation='softmax', \n",
    "                kernel_initializer=RandomUniform(), name='pi')(net)\n",
    "\n",
    "        self.model = Model(inputs=inputs, outputs=[Q, pi])\n",
    "        self.model.compile(loss=['mean_squared_error','categorical_crossentropy'], \n",
    "                           optimizer=Adam(0.001))\n",
    "        \n",
    "        \"\"\"\n",
    "        print(self.model.summary())\n",
    "        plot_model(self.model, to_file='model_plot.png', \n",
    "                show_shapes=True, show_layer_names=True)\n",
    "        img = Image.open('model_plot.png')\n",
    "        img.show()\n",
    "        \"\"\"\n",
    "\n",
    "    def train(self, examples, virtual, epoch=0):\n",
    "        \"\"\"train network and reeturn win_rate\"\"\"\n",
    "        if not virtual:\n",
    "            \"\"\"train on path batch\"\"\"\n",
    "            count = 0\n",
    "            boards_list = []; Qs_list = []; policies_list = []\n",
    "            for example in examples: \n",
    "                boards, Qs, policies = example\n",
    "                boards_list.extend(boards)\n",
    "                Qs_list.extend(Qs)\n",
    "                policies_list.extend(policies)\n",
    "\n",
    "            boards = np.reshape(boards_list, (-1,*boards[0].shape) )\n",
    "            Qs = np.reshape(Qs_list, (-1,1))\n",
    "            policies = np.reshape(policies_list, (-1,self.action_size))\n",
    "            count += len(boards) # batch size, for printing\n",
    "            start = time()\n",
    "\n",
    "            checkpointer = ModelCheckpoint(filepath=self.save_checkpoint(epoch),\n",
    "                                           save_weights_only=True,\n",
    "                                           save_best_only=True,\n",
    "                                           #monitor='loss', mode='min',\n",
    "                                           verbose=0)\n",
    "            tensorboard = TensorBoard(log_dir=f'./logging/{self.game}_cnn_{epoch}',\n",
    "                                      histogram_freq=10,\n",
    "                                      write_images=True,\n",
    "                                      batch_size=boards.size,\n",
    "                                      update_freq='batch')\n",
    "\n",
    "            self.model.fit(x = boards, y = [Qs, policies],\n",
    "                           validation_split=0.15,\n",
    "                           batch_size=boards.size,\n",
    "                           epochs = 50, shuffle=False, verbose=0,\n",
    "                           callbacks=[checkpointer, tensorboard])\n",
    "\n",
    "            end = time()\n",
    "            print(f\"fitting {count} boards: {end - start}\")\n",
    "            sys.stdout.flush()\n",
    "            #self.save_checkpoint(epoch=epoch)\n",
    "\n",
    "    def save_checkpoint(self, epoch=0): \n",
    "            folder='./checkpoints'\n",
    "            filename=f'{self.game}_cnn_{epoch}.hdf5'\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            if not os.path.exists(folder):\n",
    "                print(f\"Making Directory {folder}\")\n",
    "                os.mkdir(folder)\n",
    "            print(f'saving to {filepath}')\n",
    "            #self.model.save_weights(filepath)\n",
    "            return filepath\n",
    "\n",
    "    def load_checkpoint(self, epoch=0):\n",
    "        folder='./checkpoints'\n",
    "        filename=f'{self.game}_cnn_{epoch}.hdf5'\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"No model in path {filepath}\")\n",
    "            return\n",
    "        print(f'loading from {filepath}')\n",
    "        self.model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uM6CEpQcpu85"
   },
   "outputs": [],
   "source": [
    "import os, sys; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from time import time\n",
    "from tensorflow.compat.v1 import logging; logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, Conv2D, BatchNormalization, Activation, Flatten, Dense, Dropout, Add\n",
    "from keras.initializers import TruncatedNormal, RandomUniform\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import mse, categorical_accuracy\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class RNN():\n",
    "    def __init__(self, env, net='rnn'):\n",
    "        self.game = env.name\n",
    "        self.net = net\n",
    "        self.action_size = env.action_size # an int\n",
    "        self.board_size = env.board_size # a tuple\n",
    "\n",
    "        inputs = Input(shape=(self.board_size[0], self.board_size[1]*3) )\n",
    "        inputs_reshape = Reshape((self.board_size[0], self.board_size[1]*3, 1) )(inputs)\n",
    "        outer = Conv2D(32, kernel_size=2, strides=1, padding='same')(inputs_reshape)\n",
    "        outer = BatchNormalization(axis=-1)(outer)\n",
    "        outer = Activation('relu')(outer)\n",
    "\n",
    "        inner = outer\n",
    "        for _ in range(3): # number of blocks\n",
    "            #inner = outer\n",
    "            for _ in range(2): # layers before merge\n",
    "                inner = Conv2D(32, kernel_size=2, strides=1, padding='same')(inner)\n",
    "                inner = BatchNormalization(axis=-1)(inner)\n",
    "                inner = Activation('relu')(inner)\n",
    "            #outer = Add()([inner, outer])\n",
    "            inner = Add()([inner, outer])\n",
    "\n",
    "        pi = Conv2D(32, kernel_size=2, strides=1, padding='same')(inner)\n",
    "        pi = BatchNormalization(axis=-1)(pi)\n",
    "        pi = Activation('relu')(pi)\n",
    "        pi = Flatten()(pi)\n",
    "        pi = Dense(self.action_size, activation='softmax', name='pi_layer')(pi)\n",
    "\n",
    "        Q = Conv2D(1, kernel_size=1, strides=1, padding='same')(inner)\n",
    "        Q = BatchNormalization(axis=-1)(Q)\n",
    "        Q = Activation('relu')(Q)\n",
    "        Q = Flatten()(Q)\n",
    "        Q = Dense(1, activation='sigmoid', name='Q_layer')(Q) \n",
    "\n",
    "        self.model = Model(inputs=inputs, outputs=[Q, pi])\n",
    "        self.model.compile(loss=['mean_squared_error','categorical_crossentropy'], \n",
    "                           optimizer=Adam(0.001))\n",
    "        \n",
    "        \"\"\"\n",
    "        print(self.model.summary())\n",
    "        plot_model(self.model, to_file='model_plot.png', \n",
    "                show_shapes=True, show_layer_names=True)\n",
    "        img = Image.open('model_plot.png')\n",
    "        img.show()\n",
    "        \"\"\"\n",
    "\n",
    "    def train(self, examples, virtual, epoch=0):\n",
    "        \"\"\"train network and reeturn win_rate\"\"\"\n",
    "        if not virtual:\n",
    "            \"\"\"train on path batch\"\"\"\n",
    "            count = 0\n",
    "            boards_list = []; Qs_list = []; policies_list = []\n",
    "            for example in examples: \n",
    "                boards, Qs, policies = example\n",
    "                boards_list.extend(boards)\n",
    "                Qs_list.extend(Qs)\n",
    "                policies_list.extend(policies)\n",
    "\n",
    "            boards = np.reshape(boards_list, (-1,*boards[0].shape) )\n",
    "            Qs = np.reshape(Qs_list, (-1,1))\n",
    "            policies = np.reshape(policies_list, (-1,self.action_size))\n",
    "            count += len(boards) # batch size, for printing\n",
    "            start = time()\n",
    "\n",
    "            checkpointer = ModelCheckpoint(filepath=self.save_checkpoint(epoch),\n",
    "                                           save_weights_only=True,\n",
    "                                           save_best_only=True,\n",
    "                                           #monitor='loss', mode='min',\n",
    "                                           verbose=0)\n",
    "            tensorboard = TensorBoard(log_dir=f'./logging/{self.game}_rnn_{epoch}',\n",
    "                                      histogram_freq=10,\n",
    "                                      write_images=True,\n",
    "                                      batch_size=boards.size,\n",
    "                                      update_freq='batch')\n",
    "\n",
    "            self.model.fit(x = boards, y = [Qs, policies],\n",
    "                           validation_split=0.15,\n",
    "                           batch_size=boards.size,\n",
    "                           epochs = 50, shuffle=False, verbose=0,\n",
    "                           callbacks=[checkpointer, tensorboard])\n",
    "\n",
    "            end = time()\n",
    "            print(f\"fitting {count} boards: {end - start}\")\n",
    "            sys.stdout.flush()\n",
    "            #self.save_checkpoint(epoch=epoch)\n",
    "\n",
    "    def save_checkpoint(self, epoch=0): \n",
    "            folder='./checkpoints'\n",
    "            filename=f'{self.game}_rnn_{epoch}.hdf5'\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            if not os.path.exists(folder):\n",
    "                print(f\"Making Directory {folder}\")\n",
    "                os.mkdir(folder)\n",
    "            print(f'saving to {filepath}')\n",
    "            #self.model.save_weights(filepath)\n",
    "            return filepath\n",
    "\n",
    "    def load_checkpoint(self, epoch=0):\n",
    "        folder='./checkpoints'\n",
    "        filename=f'{self.game}_rnn_{epoch}.hdf5'\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"No model in path {filepath}\")\n",
    "            return\n",
    "        print(f'loading from {filepath}')\n",
    "        self.model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sD7dJ2QmqCsE"
   },
   "outputs": [],
   "source": [
    "\"\"\" MCTS neural network class\"\"\"\n",
    "from termcolor import colored\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "from copy import copy, deepcopy\n",
    "import numpy as np\n",
    "from random import sample\n",
    "\n",
    "def create_MCTS_instance(env, net):\n",
    "    \"\"\"create an MCTS class for the given game and network\"\"\"\n",
    "    class MCTS(env.__class__):\n",
    "        \"\"\"an MCTS using a tree structure\"\"\"\n",
    "        def __init__(self,net=None, node=None, action=None):\n",
    "            \"\"\"initialize N, Q, pi, parent, children\"\"\"\n",
    "            super().__init__()\n",
    "            if node: \n",
    "                self._copy_node(node)\n",
    "                self.step(action)\n",
    "            else: self.net = net\n",
    "            self.parent = None\n",
    "            self.children = set()\n",
    "            self.N = 0\n",
    "            self.Q_net, self.pi_net = self._predict()\n",
    "            self.Q, self.pi = copy(self.Q_net), copy(self.pi_net)\n",
    "            self.t = 1\n",
    "            self.expl = 1\n",
    "\n",
    "        def _copy_node(self, node):\n",
    "            \"\"\"copy node attributes\"\"\"        \n",
    "            \"\"\"\n",
    "            self.__dict__.update(copy(node).__dict__)\n",
    "            \"\"\"\n",
    "            self.board = copy(node.board)\n",
    "            self.turn = node.turn\n",
    "            self.done = node.done\n",
    "            self.winner = node.winner\n",
    "            self.winning_moves = copy(node.winning_moves)\n",
    "            self.last_move = node.last_move\n",
    "            self.net = node.net \n",
    "\n",
    "        def _predict(self):\n",
    "            \"\"\"get policy from net\"\"\"\n",
    "            player1_board, player2_board, next_player = self._separate_players()\n",
    "            board = np.block([player1_board, player2_board, next_player])\n",
    "            board_reshape = np.reshape(board, (-1,board.shape[0], board.shape[1]) ) \n",
    "            Q, pi = self.net.model.predict_on_batch(board_reshape)\n",
    "            Q = Q[0][0]; pi = pi[0]\n",
    "            #pi[pi==0] = 0.5\n",
    "            #pi = np.random.dirichlet(pi*self.expl + 1)\n",
    "            return Q,pi\n",
    "\n",
    "        def play(self, sims=7, expl=1):\n",
    "            \"\"\"update policy and return next state\"\"\"\n",
    "            #self.expl = expl\n",
    "            sims = min(sims, len(self.legal_moves() ))\n",
    "            \"\"\"update policies via network simulations\"\"\" \n",
    "            for _ in range(sims): self._simulate()\n",
    "\n",
    "            \"\"\"pick a move based on improved policy\"\"\"\n",
    "            children = sorted(self.children, key=lambda x: x.last())\n",
    "            next_state = np.random.choice(children, p=self.pi[self.legal_moves() ])\n",
    "            if not next_state.done: return next_state, self, None\n",
    "            else: return next_state, self, next_state._get_tree()\n",
    "\n",
    "        def _get_tree(self):\n",
    "            \"\"\"propogate from terminal, update N,Q; return path\"\"\"\n",
    "            current = self\n",
    "            nodes = []\n",
    "            while current is not None:\n",
    "                nodes.append(current)\n",
    "                current = current.parent\n",
    "            return nodes # reversed tree path\n",
    "\n",
    "        def _simulate(self):\n",
    "            \"\"\"update pi and Q via simulations\"\"\"\n",
    "            \"\"\"randomize the first move\"\"\"\n",
    "            if not self.children: self._expand()\n",
    "            # random first move for stochasticity\n",
    "            first_move = sample(self.children,1)[0]\n",
    "            leaf = first_move._leaf()\n",
    "            leaf._backpropogate(self) # reversed tree path\n",
    "\n",
    "        def _expand(self):\n",
    "            \"\"\"create children nodes\"\"\"\n",
    "            if self.children: return\n",
    "            for action in self.legal_moves():\n",
    "                #new = copy(self).step(action)\n",
    "                new = MCTS(node=self, action=action)\n",
    "                new.parent = self\n",
    "                self.children.add(new)\n",
    "                # Q and pi initialized in __init__\n",
    "\n",
    "        def _leaf(self):\n",
    "            current = self\n",
    "            \"\"\"return new leaf with updated Q\"\"\"\n",
    "            while True:\n",
    "                current.expl = self.expl\n",
    "                if current.done: return current\n",
    "                elif not current.children: return current\n",
    "                else: current = current._action()\n",
    "\n",
    "        def _action(self):\n",
    "            def UCB(child): \n",
    "                \"\"\"calculate UCB for a node\"\"\"\n",
    "                return child.Q + self.expl * self.pi[child.last()] * math.sqrt(self.N) / (child.N+1)\n",
    "\n",
    "            child = max(self.children, key = UCB)\n",
    "            return child\n",
    "\n",
    "\n",
    "        def _backpropogate(self, root):\n",
    "            \"\"\"propogate from terminal, update N,Q; return path\"\"\"\n",
    "            current = self\n",
    "            flag_N = False # don't update policy of ancestor nodes\n",
    "            flag_pi = False # don't update N of root node\n",
    "            last = current.turn\n",
    "            done = current.done\n",
    "            win = 1\n",
    "            while current is not None:\n",
    "                current._Q(last, win, done)\n",
    "                if current is root.parent: flag_pi = True \n",
    "                if current is root: flag_N = True \n",
    "                if not flag_N: current.N += 1\n",
    "                if not flag_pi: current._pi()\n",
    "                win = -win\n",
    "                current = current.parent\n",
    "\n",
    "        def _Q(self, last, win, done):\n",
    "            \"\"\"update Q values proportional to turns from terminal\"\"\"\n",
    "            # 2 is a tie, else last player won\n",
    "            if self.done: self.Q = 0 if self.winner is None else 1; return \n",
    "            for child in self.children: \n",
    "                if child.done: self.Q = -1; return;\n",
    "            Qs = [child.Q for child in self.children if child.N > 0]\n",
    "            if not Qs: return\n",
    "            else: self.Q = -sum(Qs)/len(Qs)\n",
    "\n",
    "        def _pi(self):\n",
    "            \"\"\"update policies after MCTS in last-move order\"\"\"\n",
    "            if self.done: return\n",
    "            #if self.turn > 20: self.t = (self.N + self.turn) / self.N\n",
    "            #self.t = (self.N + self.turn) / self.N\n",
    "            children = [child.N**self.t for child in self.children if child.N > 0]\n",
    "            if children: \n",
    "                for child in self.children:\n",
    "                    self.pi[child.last()] = child.N**self.t / sum(children)\n",
    "\n",
    "        def __repr__(self):\n",
    "            \"\"\"print MCTS node representation\"\"\" \n",
    "            self._print_parents()\n",
    "            self._print_self()\n",
    "            self._print_children()\n",
    "            return ''\n",
    "\n",
    "\n",
    "        def _print_parents(self):\n",
    "            \"\"\"print list of parents\"\"\"\n",
    "            print('\\nparent:')\n",
    "            if self.parent is None: print('None')\n",
    "            else:\n",
    "                parent_list = []\n",
    "                parent = self\n",
    "                while parent.parent is not None:\n",
    "                    child = parent # for color coding\n",
    "                    parent = parent.parent\n",
    "                    s = ''\n",
    "                    #s += f'\\tturn: {parent.turn}, '\n",
    "                    s += f'N: {parent.N}, '\n",
    "\n",
    "                    \"\"\"highlight Q of chosen child\"\"\"\n",
    "                    s += f'Q: '\n",
    "                    for node in sorted(parent.children, key=lambda x: x.last()):\n",
    "                        s += '\\t'\n",
    "                        if node is child: s += colored(f'{node.Q_net:2.3f}','red')\n",
    "                        else: s += f'{node.Q_net:2.3f}'\n",
    "                    for _ in range(self.action_size-len(parent.children)): s += '\\t' \n",
    "\n",
    "                    \"\"\"highlight policy of chosen child\"\"\"\n",
    "                    s += f'\\tpolicy: '\n",
    "                    for node in sorted(parent.children, key=lambda x: x.last() ):\n",
    "                        if node is child: s += colored(f'  {parent.pi_net[node.last()]:2.3f}','red')\n",
    "                        else: s += f'  {parent.pi_net[node.last()]:2.3f}'\n",
    "\n",
    "                    parent_list.append(s)\n",
    "                #string = reversed(string)\n",
    "                for parent in parent_list: print(parent,end=\"\\n\")\n",
    "\n",
    "        def _print_self(self):\n",
    "            \"\"\"print current node info\"\"\"\n",
    "            print('\\nself', end=' '); self.render()\n",
    "            s = '\\n\\t'\n",
    "            s += f' N: {self.N}, '\n",
    "            s += f'Qr: {self.Q_net:.2f}, '\n",
    "            #s += f' policy: ' + '   '.join(f\"{x:2.2f}\" for x in self.pi)\n",
    "            s += f'\\tpolicy: '\n",
    "            for node in sorted(self.children, key=lambda x: x.last() ):\n",
    "                s += f'  {self.pi_net[node.last()]:2.3f}'\n",
    "            print (s)\n",
    "\n",
    "        def _print_children(self):\n",
    "            \"\"\"print list of children\"\"\"\n",
    "            print('\\nchildren:', end=\" \")\n",
    "            if not self.children: print('None'); return ''\n",
    "            else:\n",
    "                s = ''\n",
    "                for child in sorted(self.children, key=lambda x: x.last()):\n",
    "                    s += '\\n\\t'\n",
    "                    #s += f'\\n\\tmove: {child.last()} '\n",
    "                    s += f'N: {child.N} '\n",
    "                    s += f'Q: {(child.Q_net):.2f} '\n",
    "                    #s += f'policy: ' + '   '.join(f\"{x:2.2f}\" for x in child.pi)\n",
    "\n",
    "                    s += f'\\tpolicy: '\n",
    "                    for node in sorted(child.children, key=lambda x: x.last() ):\n",
    "                        s += f'  {child.pi_net[node.last()]:2.3f}'\n",
    "                print(s)\n",
    "    return MCTS(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgQWUqxVpCud"
   },
   "outputs": [],
   "source": [
    "from time import time \n",
    "import numpy as np\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"learns to play any board game\"\"\"\n",
    "\n",
    "    def __init__(self, env, epoch=0, net='cnn'):\n",
    "        \"\"\"load or create network\"\"\"\n",
    "        self.env = env\n",
    "        if net == 'cnn': self.net = CNN(env, net=net)\n",
    "        elif net == 'rnn': self.net = RNN(env, net=net)\n",
    "        elif net == 'dnn': self.net = DNN(env, net=net)\n",
    "        elif net == 'ddpg': self.net = DDPG(env, net=net)\n",
    "        self.net.load_checkpoint(epoch=epoch)\n",
    "\n",
    "    def train(self, games=10, sims=7, epoch=0, expl=1, virtual=0):\n",
    "        \"\"\"train on (games) MCTS paths. set virtual skip learning\"\"\"\n",
    "        examples = []\n",
    "        initial_state = create_MCTS_instance(self.env,self.net)\n",
    "\n",
    "        start = time()\n",
    "        for i in range(games):\n",
    "            \"\"\"play games and add to 'examples' \"\"\"\n",
    "            node_list = []\n",
    "            next_state = initial_state \n",
    "            while True:\n",
    "                \"\"\"play best moves of a game\"\"\"\n",
    "                next_state, current_state, node_list = next_state.play(sims=sims)\n",
    "                if self._check(next_state):\n",
    "                    \"\"\"add updated states to 'examples' \"\"\"\n",
    "                    examples.append(self._make_examples(node_list)) \n",
    "                    print(f'Epoch {epoch}, Game {i+1}')\n",
    "                    break\n",
    "\n",
    "        end = time()\n",
    "        print(\"MCTS time: {}\".format(end - start))\n",
    "\n",
    "        \"\"\"train the network on the moves\"\"\"\n",
    "        self.net.train(examples, virtual, epoch=epoch)\n",
    "\n",
    "    def _check(self, node):\n",
    "        \"\"\"print last game state\"\"\"\n",
    "        if node.done:\n",
    "            clear_output(wait=True)\n",
    "            print(chr(27) + \"[2J\")\n",
    "            print(node)\n",
    "            sys.stdout.flush()\n",
    "        return node.done\n",
    "\n",
    "\n",
    "    def play(self, env):\n",
    "        \"\"\"return best move from env\"\"\"\n",
    "        board = np.block([*env._separate_players() ])\n",
    "        pi = self.net.model.predict(board.reshape((-1,*board.shape)) )[1][0]\n",
    "        # exclude illegal moves\n",
    "        pi[np.setdiff1d(range(env.action_size), env.legal_moves() )] = float('-inf')\n",
    "        return env.step(np.argmax(pi))\n",
    "\n",
    "    def _make_examples(self, node_list):\n",
    "        \"\"\"augment data, assign values, and reverse order\"\"\"\n",
    "        boards = []; Qs = []; policies = []\n",
    "        # last player won the game\n",
    "        win = 0 if node_list[0].winner is None else 1\n",
    "        for node in node_list: \n",
    "            \"\"\"split boards into player1_board and player2_board\"\"\"\n",
    "            player1_board, player2_board, next_player = node._separate_players()\n",
    "            \"\"\"get list of augmented boards, etc\"\"\"\n",
    "            player1_board, Q, policy = node.augment(player1_board, node.pi, win)\n",
    "            player2_board, Q, policy = node.augment(player2_board, node.pi, win)\n",
    "\n",
    "            \"\"\"extend list of examples\"\"\"\n",
    "            for i in range(len(player1_board) ):\n",
    "                boards.extend([np.block([player1_board[i], player2_board[i], next_player]) ])\n",
    "            Qs.extend(Q)\n",
    "            policies.extend(policy)\n",
    "            win = -win # 2nd to last player lost\n",
    "        return boards, Qs, policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7zBCeLkplHs"
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "class Arena():\n",
    "    def __init__(self, env): self.env = env\n",
    "\n",
    "    def compete(self, player1, player2, runs):\n",
    "        \"\"\"return (player1) win rate over (runs) games\"\"\"\n",
    "       # print('=========================')\n",
    "       # print('competing agents')\n",
    "       # print('=========================')\n",
    "        wins_player1 = 0\n",
    "        wins_player2 = 0\n",
    "\n",
    "        turn = choice([1,2])\n",
    "        for _ in range(runs):\n",
    "            \"\"\"play (runs) games\"\"\"\n",
    "            while True:\n",
    "                if turn == 1:\n",
    "                    self.env = player1.play(self.env)\n",
    "                    if self._check('player1'):\n",
    "                        wins_player1 += 1\n",
    "                        turn = choice([1,2])\n",
    "                        break\n",
    "\n",
    "                if turn == 2:\n",
    "                    self.env = player2.play(self.env)\n",
    "                    if self._check('player2'):\n",
    "                        wins_player2 += 1\n",
    "                        turn = choice([1,2])\n",
    "                        break\n",
    "\n",
    "        return wins_player1/runs \n",
    "\n",
    "    def test(self, player, runs):\n",
    "        \"\"\"return win rate for player 1\"\"\"\n",
    "        class testser():\n",
    "            def __init__(self): pass\n",
    "            def play(self, env): return env.step(choice(env.legal_moves() ))\n",
    "            \n",
    "        return self.compete(player, testser(), runs)\n",
    "        \n",
    "    def _check(self, player):\n",
    "        \"\"\"detect gameover, \"\"\" \n",
    "        if self.env.done:\n",
    "            #self.env.render()\n",
    "            self.env.reset()\n",
    "            #print(player + ' self.wins!')\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgkHyj9ZoyTb"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category= FutureWarning)\n",
    "\n",
    "from time import sleep\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def env(game):\n",
    "    if game == 'tictactoe':\n",
    "        return TictactoeEnv()\n",
    "    elif game == 'connect4':\n",
    "        return Connect4Env()\n",
    "\n",
    "class Configs():\n",
    "    def __init__(self, game, net, start, epochs, games):\n",
    "        self.draws = 4 # when has learning has stopped\n",
    "        self.tests = 30 # games in each test\n",
    "        self.threshold = .55 # win rate before updating\n",
    "        self.epochs = epochs # epochs to train networks\n",
    "        self.games = games # games to train on\n",
    "\n",
    "        # epoch to start from\n",
    "        if start == 'last': self.start = self.last(game,net)\n",
    "        else: self.start = int(start)\n",
    "        # MCTS searches each move\n",
    "        if game == 'tictactoe': self.sims = 4 \n",
    "        elif game == 'connect4': self.sims = 14\n",
    "\n",
    "    def last(self, game, net): # continue from last epoch\n",
    "        i = 0\n",
    "        while True:\n",
    "            filepath = f'./checkpoints/{game}_{net}_{i+1}.hdf5'\n",
    "            if os.path.exists(filepath): i += 1\n",
    "            else: return i+1\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"trains an agent to play {game} with {net}.\"\"\"\n",
    "    game = args.game\n",
    "    net = args.net\n",
    "    configs = Configs(game, net, args.start, args.epochs, args.games)\n",
    "    arena = Arena(env(game) )\n",
    "    #new = Agent(TictactoeEnv(),et='rnn',virtual=1)\n",
    "\n",
    "    win_rates = []\n",
    "    test_rates = []\n",
    "\n",
    "    old = Agent(env(game), net=net, epoch=configs.start-1)\n",
    "    new = Agent(env(game), net=net, epoch=configs.start-1)\n",
    "\n",
    "    for epoch in range(configs.start, configs.start + configs.epochs):\n",
    "        draws = 0\n",
    "        win_rate = 0\n",
    "        while win_rate < configs.threshold and draws < configs.draws: \n",
    "            draws += 1\n",
    "            new.train(games=configs.games, sims=configs.sims, epoch=epoch, expl=1)\n",
    "\n",
    "            win_rate = arena.compete(new, old, configs.tests)\n",
    "            win_rates.append(win_rate)\n",
    "            test_rate = arena.test(new, configs.tests)\n",
    "            test_rates.append(test_rate)\n",
    "\n",
    "            print(\"win rate: {}\".format(win_rate))\n",
    "            print(\"test rate: {}\".format(test_rate))\n",
    "            print(\"draws: {}\".format(draws))\n",
    "            print(\"epoch: {}\".format(epoch))\n",
    "            sys.stdout.flush()\n",
    "            #new.train(games=configs.games, sims=configs.sims, epoch=epoch, expl=1, virtual=True)\n",
    "        old = Agent(env(game), net=net, epoch=epoch)\n",
    "        #old = Agent(TictactoeEnv(),et='rnn', epoch=0, virtual=True)\n",
    "\n",
    "    plt.plot(win_rates, 'g') \n",
    "    plt.plot(test_rates, 'r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66711,
     "status": "error",
     "timestamp": 1568632912198,
     "user": {
      "displayName": "Tor Saxberg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDMqlWuXygkDE3LzKrVH8_fetTuNkbD0ke0p24O=s64",
      "userId": "17428565500638805826"
     },
     "user_tz": 420
    },
    "id": "kN9Rub3xpAVV",
    "outputId": "50b24c04-eb90-4d7a-80aa-424168e9f6dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2J\n",
      "\n",
      "parent:\n",
      "N: 1, Q: \t\u001b[31m0.000\u001b[0m\t\t\t\t\t\t\t\t\tpolicy: \u001b[31m  0.214\u001b[0m\n",
      "N: 2, Q: \t\u001b[31m0.000\u001b[0m\t0.000\t\t\t\t\t\t\t\tpolicy: \u001b[31m  0.050\u001b[0m  0.415\n",
      "N: 1, Q: \t0.005\t\u001b[31m0.000\u001b[0m\t0.000\t\t\t\t\t\t\tpolicy:   0.069\u001b[31m  0.113\u001b[0m  0.238\n",
      "N: 1, Q: \t0.000\t0.006\t\u001b[31m0.000\u001b[0m\t0.000\t\t\t\t\t\tpolicy:   0.069  0.148\u001b[31m  0.137\u001b[0m  0.221\n",
      "N: 2, Q: \t0.000\t0.000\t\u001b[31m0.000\u001b[0m\t0.000\t0.000\t\t\t\t\tpolicy:   0.081  0.066\u001b[31m  0.167\u001b[0m  0.263  0.100\n",
      "N: 1, Q: \t\u001b[31m0.000\u001b[0m\t0.000\t0.000\t0.000\t0.000\t0.000\t\t\t\tpolicy: \u001b[31m  0.092\u001b[0m  0.039  0.101  0.105  0.297  0.195\n",
      "N: 14, Q: \t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t\u001b[31m0.000\u001b[0m\t\t\tpolicy:   0.115  0.101  0.101  0.177  0.210  0.116\u001b[31m  0.099\u001b[0m\n",
      "N: 16, Q: \t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t\u001b[31m0.000\u001b[0m\t\tpolicy:   0.129  0.044  0.150  0.037  0.143  0.214  0.113\u001b[31m  0.167\u001b[0m\n",
      "N: 0, Q: \t0.000\t0.258\t0.000\t0.000\t\u001b[31m0.000\u001b[0m\t0.099\t0.033\t0.000\t0.000\tpolicy:   0.094  0.121  0.153  0.089\u001b[31m  0.026\u001b[0m  0.118  0.133  0.162  0.104\n",
      "\n",
      "self \n",
      "turn: 9, last: [2, 0]\n",
      "\t0 | X | X \u001b[31m| O\u001b[0m |\n",
      "\t1 | O \u001b[31m| O\u001b[0m | X |\n",
      "\t2 \u001b[32m| O\u001b[0m | O | X |\n",
      "\t   _   _   _ \n",
      "\t   0   1   2 \n",
      "Game Over!\n",
      "O is the winner\n",
      "\n",
      "\t N: 1, Qr: 0.00, \tpolicy: \n",
      "\n",
      "children: None\n",
      "\n",
      "Epoch 21, Game 20\n",
      "MCTS time: 1.9906055927276611\n",
      "saving to ./checkpoints/tictactoe_rnn_21.hdf5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'input_2' with dtype float and shape [?,3,9]\n\t [[{{node input_2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4cf1b9258ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-6ed15fbeb217>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mwin_rate\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdraws\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraws\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdraws\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mwin_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marena\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3f872c03fd93>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, games, sims, epoch, expl, virtual)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"train the network on the moves\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvirtual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-99b9181b86ab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, examples, virtual, epoch)\u001b[0m\n\u001b[1;32m     94\u001b[0m                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                            \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                            callbacks=[checkpointer, tensorboard])\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    222\u001b[0m                         \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m                     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m                     \u001b[0msummary_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'input_2' with dtype float and shape [?,3,9]\n\t [[node input_2 (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541) ]]\n\nOriginal stack trace for 'input_2':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-26b258fad2c6>\", line 19, in <module>\n    try: main(args)\n  File \"<ipython-input-8-6ed15fbeb217>\", line 48, in main\n    new = Agent(env(game), net=net, epoch=configs.start-1)\n  File \"<ipython-input-6-3f872c03fd93>\", line 13, in __init__\n    elif net == 'rnn': self.net = RNN(env, net=net)\n  File \"<ipython-input-4-8fc30eb3c3b2>\", line 23, in __init__\n    inputs = Input(shape=(self.board_size[0], self.board_size[1]*3) )\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\", line 178, in Input\n    input_tensor=tensor)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\", line 87, in __init__\n    name=self.name)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 541, in placeholder\n    x = tf.placeholder(dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 6262, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # you can run '$python3 main.py' with no arguments\n",
    "    args = {}\n",
    "    args['start'] = 'last'\n",
    "    args['epoch'] = 10\n",
    "    args['net'] = 'rnn'\n",
    "    args['game'] = 'tictactoe'\n",
    "    \n",
    "    class Args():\n",
    "        def __init__(self):\n",
    "            self.start = 'last'\n",
    "            self.epochs = 20\n",
    "            self.net = 'rnn'\n",
    "            self.game = 'tictactoe'\n",
    "            self.games = 20\n",
    "    args = Args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Alpha Zero.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
